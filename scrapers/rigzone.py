"""
RigZone Scraper
===============
Website: https://www.rigzone.com/

Selectors:
- Date: <span class="date">December 19, 2025</span>
- Article Link: <a href="https://www.rigzone.com/news/...">
- Article Content: <p> tags (skip comment disclaimer)

Logic:
1. Visit headlines page
2. Check date - only scrape if today or yesterday
3. Check if link already in CSV - skip if already scraped
4. Scrape article content
5. Save to CSV
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd
import os
from datetime import datetime, timedelta
from scrapers.utils import standardize_date, clean_content, save_to_csv, get_existing_links as get_links, fetch_articles_parallel

SOURCE = 'rigzone'
BASE_URL = 'https://www.rigzone.com'
NEWS_URLS = [
    'https://www.rigzone.com/news/industry_headlines/',
    'https://www.rigzone.com/news/industry_headlines/2/',
    'https://www.rigzone.com/news/industry_headlines/3/',
    'https://www.rigzone.com/news/industry_headlines/4/',
    'https://www.rigzone.com/news/industry_headlines/5/',
]
CSV_FILE = os.path.join(os.path.dirname(__file__), 'articles.csv')
TIMEOUT = 30  # seconds

headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'accept-language': 'en-US,en;q=0.9',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
}


def get_existing_links():
    """Load existing article links to avoid duplicates"""
    return get_links(CSV_FILE, SOURCE)


def parse_date(date_text):
    """Parse date from text like 'December 19, 2025' or '19-dec-2025'"""
    try:
        # Try format: December 19, 2025
        return datetime.strptime(date_text.strip(), '%B %d, %Y').date()
    except:
        pass
    
    try:
        # Try format from URL: 19-dec-2025
        return datetime.strptime(date_text.strip(), '%d-%b-%Y').date()
    except:
        return None


def extract_date_from_url(url):
    """Extract date from URL like /news/wire/oil_posts_second_weekly_decline-19-dec-2025-182583-article/"""
    import re
    match = re.search(r'-(\d{1,2}-[a-z]{3}-\d{4})-', url.lower())
    if match:
        return match.group(1)
    return None


def is_recent(date_text):
    """Check if date is recent - DISABLED: collecting all articles for training"""
    # DATE FILTERING DISABLED - Collecting all articles for NLP training
    return True


def get_article_links(soup):
    """Extract article links from the main page - only for recent articles"""
    articles = []
    seen_links = set()
    
    # Find all news article links
    all_links = soup.find_all('a', href=True)
    
    for a_tag in all_links:
        href = a_tag.get('href', '')
        
        # Only process article links
        if '/news/' not in href or 'article' not in href:
            continue
        
        # Get full link and remove hash fragments
        link = href if href.startswith('http') else BASE_URL + href
        link = link.split('#')[0]  # Remove #hasComments etc.
        
        # Skip duplicates
        if link in seen_links:
            continue
        seen_links.add(link)
        
        # Extract date from URL (format: -19-dec-2025-)
        date_from_url = extract_date_from_url(link)
        
        if date_from_url:
            # Format date nicely
            parsed = parse_date(date_from_url)
            if parsed:
                date_text = parsed.strftime('%B %d, %Y')
                
                articles.append({
                    'link': link,
                    'date': date_text
                })
                print(f"  âœ“ Found ({date_text}): {link[:50]}...")
        else:
            print(f"  ? No date found: {link[:50]}...")
    
    return articles


def get_article_content(url, max_retries=3):
    """Scrape article content from article page with retry logic"""
    import time
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=TIMEOUT)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            paragraphs = soup.find_all('p')
            content_parts = []
            
            for p in paragraphs:
                text = p.get_text().strip()
                # Skip empty and comment disclaimer text
                if not text:
                    continue
                if 'Generated by readers' in text or 'comments included herein' in text:
                    continue
                content_parts.append(text)
            
            return '\n'.join(content_parts)
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                print(f"  Timeout, retrying... ({attempt + 1}/{max_retries})")
                time.sleep(2)
            else:
                print(f"Error scraping {url}: Timeout after {max_retries} attempts")
                return ''
        except Exception as e:
            print(f"Error scraping {url}: {e}")
            return ''


def save_articles(articles):
    """Save articles to CSV with clean formatting"""
    save_to_csv(articles, CSV_FILE, SOURCE)


def scrape(existing_links=None):
    """
    Main scrape function
    
    1. Fetch headlines page
    2. Filter by date (today/yesterday only)
    3. Skip already scraped links
    4. Scrape content & summarize
    5. Save to CSV
    """
    if existing_links is None:
        existing_links = get_existing_links()
    
    import time
    print(f"Already scraped: {len(existing_links)} articles")
    
    all_articles = []
    
    for news_url in NEWS_URLS:
        print(f"\n--- Checking {news_url} ---")
        try:
            response = requests.get(news_url, headers=headers, timeout=TIMEOUT)
            print(f"Status: {response.status_code}")
        except Exception as e:
            print(f"Error fetching page: {e}")
            continue
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = get_article_links(soup)
        all_articles.extend(articles)
        time.sleep(1)  # Be nice to the server
    
    # Remove duplicates
    seen = set()
    unique_articles = []
    for article in all_articles:
        if article['link'] not in seen:
            seen.add(article['link'])
            unique_articles.append(article)
    
    print(f"\nFound {len(unique_articles)} total articles")
    
    # Filter out already scraped
    new_articles = [a for a in unique_articles if a['link'] not in existing_links]
    print(f"New articles to scrape: {len(new_articles)}")
    
    if not new_articles:
        print("No new articles to scrape.")
        return []
    
    # Fetch articles in parallel for speed
    results = fetch_articles_parallel(
        articles=new_articles,
        fetch_func=get_article_content,
        max_workers=10,
        source_name=SOURCE,
        standardize=True
    )
    
    return results


# For standalone testing
if __name__ == "__main__":
    print("=" * 60)
    print(f"RigZone Scraper - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)
    
    articles = scrape()
    print(f"\nScraped {len(articles)} new articles")
    
    if articles:
        save_articles(articles)
        print("\n--- Results ---")
        for a in articles:
            print(f"\nDate: {a['date']}")
            print(f"Link: {a['link']}")
            print(f"Content: {a['content'][:200]}..." if a['content'] else "No content")


